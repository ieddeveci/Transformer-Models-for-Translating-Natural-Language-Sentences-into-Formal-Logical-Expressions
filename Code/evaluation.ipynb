{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09308eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "from nltk.sem.logic import Expression\n",
    "from nltk.inference import Prover9\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import concurrent.futures\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Preprocess and Evaluation Metrics\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "# Converts FOL symbols to tokenizable representation for preprocessing.\n",
    "def fol_preprocess(expression):\n",
    "    replacements = {\n",
    "        '∀': ' FORALL ',\n",
    "        '∃': ' EXISTS ',\n",
    "        '¬': 'NOT ',\n",
    "        '∧': 'AND',\n",
    "        '⊕': 'XOR',\n",
    "        '∨': 'OR',\n",
    "        '→': 'THEN',\n",
    "        '↔': 'IFF',\n",
    "    }\n",
    "    for symbol, replacement in replacements.items():\n",
    "        expression = expression.replace(symbol, replacement)\n",
    "    return expression\n",
    "\n",
    "# Converts tokenizable representation back to FOL syntax for postprocessing.\n",
    "def fol_postprocess(text):\n",
    "    inv_replacements = {\n",
    "        'FORALL': '∀',\n",
    "        'EXISTS': '∃',\n",
    "        'NOT': '¬',\n",
    "        'AND': '∧',\n",
    "        'XOR': '⊕',\n",
    "        'OR': '∨',\n",
    "        'THEN': '→',\n",
    "        'IFF': '↔',\n",
    "    }\n",
    "    for replacement, symbol in inv_replacements.items():\n",
    "        text = text.replace(replacement, symbol)\n",
    "    return text\n",
    "\n",
    "# Converts tokenized representations back to standard FOL syntax excluding XOR, which is not supported by Prover9.\n",
    "def fol_postprocessXOR(text):\n",
    "    inv_replacements = {\n",
    "        'FORALL': '∀',\n",
    "        'EXISTS': '∃',\n",
    "        'NOT': '¬',\n",
    "        'AND': '∧',\n",
    "        'XOR': '∨',\n",
    "        'OR': '∨',\n",
    "        'THEN': '→',\n",
    "        'IFF': '↔',\n",
    "    }\n",
    "    for replacement, symbol in inv_replacements.items():\n",
    "        text = text.replace(replacement, symbol)\n",
    "    return text\n",
    "\n",
    "# Normalization\n",
    "def nl_preprocess(sentence):\n",
    "    return unicodedata.normalize('NFKC', sentence.lower())\n",
    "\n",
    "# Converts a logical expression into NLTK-compatible format by replacing symbols with NLTK equivalents.\n",
    "def to_nltk(expression):\n",
    "    replacements = {\n",
    "        '∀': ' all ',\n",
    "        '∃': ' exists ',\n",
    "        '¬': '-',\n",
    "        '∧': '&',\n",
    "        '⊕': '!=',\n",
    "        '∨': '|',\n",
    "        '→': '->',\n",
    "        '↔': '<->',\n",
    "        'FORALL': ' all ',\n",
    "        'EXISTS': ' exists ',\n",
    "        'NOT': ' - ',\n",
    "        'AND': '&',\n",
    "        'XOR': '!=',\n",
    "        'OR': '|',\n",
    "        'THEN': '->',\n",
    "        'IFF': '<->',\n",
    "    }\n",
    "    for symbol, nltk in replacements.items():\n",
    "        expression = re.sub(re.escape(symbol), nltk, expression)\n",
    "    return expression\n",
    "\n",
    "# Checks if a logical expression is well-formed.\n",
    "def check_grammar(expression):\n",
    "    nltk_expression = to_nltk(expression)\n",
    "    try:\n",
    "        parsed_expr = Expression.fromstring(nltk_expression)\n",
    "        return len(parsed_expr.free()) == 0\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Replaces predicate names in logical expressions with numeric placeholders.\n",
    "def predicate_abstraction(expr1, expr2):\n",
    "    def abstract_independently(expr):\n",
    "        seen = {}\n",
    "        counter = 1\n",
    "        for match in re.finditer(r'\\b([a-zA-Z0-9_]+)\\(', expr):\n",
    "            pred = match.group(1)\n",
    "            if pred not in seen:\n",
    "                seen[pred] = str(counter)\n",
    "                counter += 1\n",
    "        return seen\n",
    "\n",
    "    pred_map1 = abstract_independently(expr1)\n",
    "    pred_map2 = abstract_independently(expr2)\n",
    "\n",
    "    shared_preds = set(pred_map1) & set(pred_map2)\n",
    "    aligned_map = {}\n",
    "    next_id = max(len(pred_map1), len(pred_map2)) + 1\n",
    "    for pred in shared_preds:\n",
    "        aligned_map[pred] = str(next_id)\n",
    "        next_id += 1\n",
    "\n",
    "    def apply_abstraction(expr, local_map):\n",
    "        combined = {**local_map}\n",
    "        for pred in aligned_map:\n",
    "            if pred in combined:\n",
    "                combined[pred] = aligned_map[pred]\n",
    "\n",
    "        for pred in sorted(combined, key=len, reverse=True):\n",
    "            expr = re.sub(r'\\b' + re.escape(pred) + r'\\(', combined[pred] + '(', expr)\n",
    "        return expr\n",
    "\n",
    "    return apply_abstraction(expr1, pred_map1), apply_abstraction(expr2, pred_map2)\n",
    "\n",
    "# Compares the abstracted structures of two logical expressions.\n",
    "def compare_structure(expr1, expr2):\n",
    "    abstracted_expr1, abstracted_expr2 = predicate_abstraction(expr1, expr2)\n",
    "    exp1 = to_nltk(abstracted_expr1)\n",
    "    exp2 = to_nltk(abstracted_expr2)\n",
    "\n",
    "    try:\n",
    "        parsed_expr1 = Expression.fromstring(exp1)\n",
    "        parsed_expr2 = Expression.fromstring(exp2)\n",
    "        return parsed_expr1 == parsed_expr2\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Counts the number of unique predicates in a logical expression.\n",
    "def count_predicates(expression):\n",
    "    predicate_pattern = r'\\b[A-Za-z]+\\d*\\([^()]*\\)'\n",
    "    predicates = re.findall(predicate_pattern, expression)\n",
    "    predicate_set = {re.sub(r'\\([^()]*\\)', '', pred).strip() for pred in predicates}\n",
    "    return len(predicate_set)\n",
    "\n",
    "# Checks if two logical expressions are equivalent using Prover9. Prover9 must be installed: https://www.cs.unm.edu/~mccune/prover9/\n",
    "def _equivalence_logic(expr1, expr2):\n",
    "    os.environ['PROVER9'] = 'Prover9-Mace4/bin'\n",
    "    prover = Prover9()\n",
    "    abstracted_expr1, abstracted_expr2 = predicate_abstraction(expr1, expr2)\n",
    "    expr1 = Expression.fromstring(to_nltk(abstracted_expr1))\n",
    "    expr2 = Expression.fromstring(to_nltk(abstracted_expr2))\n",
    "    return expr1.equiv(expr2)\n",
    "\n",
    "def equivalence(expr1_str, expr2_str, timeout=30):\n",
    "    try:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n",
    "            future = executor.submit(_equivalence_logic, expr1_str, expr2_str)\n",
    "            return future.result(timeout=timeout)\n",
    "    except concurrent.futures.TimeoutError:\n",
    "        logging.warning(\"equivalence() timed out.\")\n",
    "        return \"equivalence could not halt\"\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b972eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Tokenization and loading the dataset with preprocessing\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "dataset_path = \"\"\n",
    "\n",
    "def tokenize_data(dataset, tokenizer):\n",
    "    def tokenize_function(examples):\n",
    "        model_inputs = tokenizer(\n",
    "            examples['NL'],\n",
    "            max_length=256,\n",
    "            truncation=True,\n",
    "            padding='max_length'\n",
    "        )\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(\n",
    "                examples['FOL'],\n",
    "                max_length=256,\n",
    "                truncation=True,\n",
    "                padding='max_length'\n",
    "            )\n",
    "        model_inputs['labels'] = labels['input_ids']\n",
    "        return model_inputs\n",
    "    return dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "def load_train_val_test_data(tokenizer):\n",
    "    dataset = pd.read_excel(dataset_path)\n",
    "    train_data, test_data = train_test_split(dataset, test_size=0.03, random_state=42)\n",
    "    train_data, validation_data = train_test_split(train_data, test_size=0.1, random_state=42)\n",
    "\n",
    "    print(f\"Train Dataset size: {len(train_data)}\")\n",
    "    print(f\"Validation Dataset size: {len(validation_data)}\")\n",
    "    print(f\"Test Dataset size: {len(test_data)}\")\n",
    "\n",
    "    train_data = Dataset.from_pandas(train_data)\n",
    "    validation_data = Dataset.from_pandas(validation_data)\n",
    "    test_data = Dataset.from_pandas(test_data)\n",
    "\n",
    "    train_data = train_data.map(lambda x: {'NL': 'translate English to First-order Logic: ' + nl_preprocess(x['NL_sentence']), 'FOL': fol_preprocess(x['FOL_expression'])})\n",
    "    validation_data = validation_data.map(lambda x: {'NL': 'translate English to First-order Logic: ' + nl_preprocess(x['NL_sentence']), 'FOL': fol_preprocess(x['FOL_expression'])})\n",
    "    test_data = test_data.map(lambda x: {'NL': 'translate English to First-order Logic: ' + nl_preprocess(x['NL_sentence']), 'FOL': fol_preprocess(x['FOL_expression'])})\n",
    "\n",
    "    train_data = tokenize_data(train_data, tokenizer)\n",
    "    validation_data = tokenize_data(validation_data, tokenizer)\n",
    "    test_data = tokenize_data(test_data, tokenizer)\n",
    "\n",
    "    return train_data, validation_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f4e139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Model Preparation and Train, Val, Test Data\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "model_path = ''\n",
    "pretrained_model = ''\n",
    "max_length = 256\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = load_train_val_test_data(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f261b294",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "# Evaluation - Deterministic Inference and Stochastic Reranking\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "def deterministic_eval(dataset, model, tokenizer, device):\n",
    "    metrics = {\n",
    "        'accurate': 0,\n",
    "        'formally_accurate': 0,\n",
    "        'equivalent': 0,\n",
    "        'well_formed': 0,\n",
    "        'ill_formed': 0,\n",
    "        'false_cases': []\n",
    "    }\n",
    "\n",
    "    for example in dataset:\n",
    "        input_ids = torch.tensor(example['input_ids']).unsqueeze(0).to(device)\n",
    "        label_ids = torch.tensor(example['labels']).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(input_ids, max_length=max_length)\n",
    "\n",
    "        nl_input = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "        pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        label_text = tokenizer.decode(label_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        pred = fol_postprocess(pred_text)\n",
    "        ref = fol_postprocess(label_text)\n",
    "        predXOR = fol_postprocessXOR(pred_text)\n",
    "        refXOR = fol_postprocessXOR(label_text)\n",
    "        abstracted_pred, abstracted_ref = predicate_abstraction(pred, ref)\n",
    "\n",
    "        print(\"\\n--- Evaluation Instance ---\")\n",
    "        print(f\"Natural Language Input:  {nl_input}\")\n",
    "        print(f\"Generated Expression:    {pred}\")\n",
    "        print(f\"Reference Expression:    {ref}\")\n",
    "        print(f\"Abstracted Prediction:   {abstracted_pred}\")\n",
    "        print(f\"Abstracted Reference:    {abstracted_ref}\")\n",
    "\n",
    "        is_well_formed_pred = check_grammar(pred)\n",
    "        is_well_formed_ref = check_grammar(ref)\n",
    "        print(f\"Prediction Well-Formed:  {is_well_formed_pred}\")\n",
    "        print(f\"Reference Well-Formed:   {is_well_formed_ref}\")\n",
    "\n",
    "        if is_well_formed_pred and is_well_formed_ref:\n",
    "            metrics['well_formed'] += 1\n",
    "\n",
    "            is_exact = (pred == ref)\n",
    "            is_formal = compare_structure(pred, ref)\n",
    "            is_equiv = equivalence(predXOR, refXOR)\n",
    "\n",
    "            print(f\"Exact Match:             {is_exact}\")\n",
    "            print(f\"Formally Accurate:       {is_formal}\")\n",
    "            print(f\"Logically Equivalent:    {is_equiv}\")\n",
    "\n",
    "            if is_exact:\n",
    "                metrics['accurate'] += 1\n",
    "            if is_formal:\n",
    "                metrics['formally_accurate'] += 1\n",
    "            elif is_equiv:\n",
    "                metrics['equivalent'] += 1\n",
    "            else:\n",
    "                metrics['false_cases'].append({\n",
    "                    'input_ids': input_ids.squeeze(0).cpu(),\n",
    "                    'reference': ref,\n",
    "                    'nl_sentence': nl_input,\n",
    "                    'prediction': pred\n",
    "                })\n",
    "        else:\n",
    "            print(\"One or both expressions ill-formed.\")\n",
    "            metrics['ill_formed'] += 1\n",
    "            metrics['false_cases'].append({\n",
    "                'input_ids': input_ids.squeeze(0).cpu(),\n",
    "                'reference': ref,\n",
    "                'nl_sentence': nl_input,\n",
    "                'prediction': pred\n",
    "            })\n",
    "\n",
    "    total = len(dataset)\n",
    "    # denom = max(total - metrics['ill_formed'], 1) # For omitting ill-formed expressions\n",
    "    denom = total\n",
    "\n",
    "    return {\n",
    "        'accuracy_rate': metrics['accurate'] / denom,\n",
    "        'formal_rate': metrics['formally_accurate'] / denom,\n",
    "        'equivalent_rate': (metrics['formally_accurate'] + metrics['equivalent']) / denom,\n",
    "        'well_formed_rate': metrics['well_formed'] / total,\n",
    "        'false_cases': metrics['false_cases'],\n",
    "        'det_correct': metrics['formally_accurate'] + metrics['equivalent'],\n",
    "        'det_total': denom\n",
    "    }\n",
    "\n",
    "def stochastic_reranking(false_cases, model, tokenizer, device, num_samples=10):\n",
    "    improved_count = 0\n",
    "    results = []\n",
    "    corrected_cases = []\n",
    "\n",
    "    for case in false_cases:\n",
    "        input_ids = case['input_ids'].unsqueeze(0).to(device)\n",
    "        nl_sentence = case['nl_sentence']\n",
    "        reference = case['reference']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids,\n",
    "                max_length=max_length,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                temperature=0.8,\n",
    "                num_return_sequences=num_samples\n",
    "            )\n",
    "\n",
    "        successful = False\n",
    "        best_candidate = None\n",
    "        samples = []\n",
    "\n",
    "        for output in outputs:\n",
    "            pred = fol_postprocess(tokenizer.decode(output, skip_special_tokens=True))\n",
    "            well_formed = check_grammar(pred)\n",
    "            equivalent = equivalence(pred, reference)\n",
    "\n",
    "            samples.append({\n",
    "                'prediction': pred,\n",
    "                'well_formed': well_formed,\n",
    "                'equivalent': equivalent\n",
    "            })\n",
    "\n",
    "            if equivalent:\n",
    "                if not successful:\n",
    "                    best_candidate = pred\n",
    "                successful = True\n",
    "\n",
    "        if successful:\n",
    "            improved_count += 1\n",
    "            print(\"\\n--- CORRECTION FOUND ---\")\n",
    "            print(f\"Natural Language Sentence: {nl_sentence}\")\n",
    "            print(f\"Reference FOL:             {case['reference']}\")\n",
    "            print(f\"Incorrect Prediction:      {case['prediction']}\")\n",
    "            print(f\"Corrected Prediction:      {best_candidate}\")\n",
    "\n",
    "            corrected_cases.append({\n",
    "                'nl_sentence': nl_sentence,\n",
    "                'previous_prediction': case['prediction'],\n",
    "                'reference': case['reference'],\n",
    "                'corrected_prediction': best_candidate\n",
    "            })\n",
    "\n",
    "        results.append({\n",
    "            'nl_sentence': nl_sentence,\n",
    "            'reference': reference,\n",
    "            'samples': samples\n",
    "        })\n",
    "\n",
    "    improvement_rate = improved_count / len(false_cases) if false_cases else 0\n",
    "    return improvement_rate, results, corrected_cases, improved_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d30321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model():\n",
    "    base_results = deterministic_eval(test_dataset, model, tokenizer, device)\n",
    "\n",
    "    output_lines = []\n",
    "    output_lines.append(f\"Model Path: {model_path}\")\n",
    "    output_lines.append(f\"Tokenizer: {model_name}\")\n",
    "    output_lines.append(\"\\n--- Deterministic Evaluation ---\")\n",
    "    output_lines.append(f\"Well-formed:       {base_results['well_formed_rate']:.4f}\")\n",
    "    output_lines.append(f\"Exact match:       {base_results['accuracy_rate']:.4f}\")\n",
    "    output_lines.append(f\"Formal match:      {base_results['formal_rate']:.4f}\")\n",
    "    output_lines.append(f\"Equivalence:       {base_results['equivalent_rate']:.4f}\")\n",
    "\n",
    "    improvement_rate, samples, corrected_cases, corrected_count = stochastic_reranking(\n",
    "        base_results['false_cases'], model, tokenizer, device\n",
    "    )\n",
    "\n",
    "    corrected_total = base_results['det_correct'] + corrected_count\n",
    "    total_evaluated = base_results['det_total']\n",
    "    corrected_equiv_rate = corrected_total / total_evaluated\n",
    "\n",
    "    output_lines.append(\"\\n--- Stochastic Reranking ---\")\n",
    "    output_lines.append(f\"Deterministic Score:     {base_results['equivalent_rate']:.4f}\")\n",
    "    output_lines.append(f\"Corrected Predictions:   {corrected_count}\")\n",
    "    output_lines.append(f\"Final Score:             {corrected_equiv_rate:.4f}\")\n",
    "    output_lines.append(f\"Overall Total Evaluated: {total_evaluated}\")\n",
    "\n",
    "    model_name = pretrained_model.split(\"/\")[-1].replace(\"-\", \"_\")\n",
    "    output_filename = f\"{model_name}_evaluation_results.txt\"\n",
    "    output_path = output_filename\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for line in output_lines:\n",
    "            f.write(line + \"\\n\")\n",
    "\n",
    "    return base_results, improvement_rate, samples, corrected_cases\n",
    "\n",
    "evaluate_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
